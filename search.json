[
  {
    "objectID": "sub6.html",
    "href": "sub6.html",
    "title": "Final Project",
    "section": "",
    "text": "üìÑ Final Project PDF"
  },
  {
    "objectID": "sub6.html#shiny-app",
    "href": "sub6.html#shiny-app",
    "title": "Final Project",
    "section": "Shiny App",
    "text": "Shiny App\nhttps://malashiny.shinyapps.io/test-1/"
  },
  {
    "objectID": "sub4.html",
    "href": "sub4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "Meal Cost for Two in Houston and Dallas This chart illustrates the cost of a mid-range, three-course meal for two people in Houston, TX and Dallas, TX. The data is based on current cost-of-living figures, highlighting the price differences between these two major cities in Texas.\nHouston has a slightly higher average cost of $53.91 for a meal for two, while Dallas shows a lower cost at $44.92. The width of each bar represents an arbitrary ‚Äúmarket share‚Äù in this chart to demonstrate variable-width column charts, though this could represent the size of the dining market or the popularity of restaurants in these cities. The contrasting colors make it easy to distinguish between the two cities and quickly compare the meal prices. ‚Äî\n\n\n\n\n\n\n\n\n\nMeal Cost Comparison Across Selected US Cities This bar chart compares the cost of a mid-range, three-course meal for two people across four selected cities in the United States: Houston, Dallas, Austin, and Chicago. The data highlights differences in meal pricing across these urban areas:\nHouston has the highest meal cost at approximately $53.91. Dallas follows closely with a meal cost of $44.92. Austin and Chicago both have meal costs around $50, slightly below that of Houston. The custom colors and rotated x-axis labels provide an easy-to-read comparison of meal costs in these cities, helping to visualize regional differences in cost of living. ‚Äî\n\n\n\n\n\n\n\n\n\nCost Comparison for Different Categories in Houston This bar chart compares the costs of three key categories in Houston, Texas: Meal for 2, 1-bedroom Apartment Rent, and Gasoline (per liter). Due to the large disparity in values between these categories, the chart uses a logarithmic scale on the y-axis to provide a clearer comparison:\nThe cost of renting a 1-bedroom apartment in Houston is significantly higher, around $2,311.84. The cost of a mid-range meal for two people is approximately $53.91, much lower in comparison. The price of gasoline per liter is around $1.17. ‚Äî\n\n\n\n\n\n\n\n\n\nCost Comparison for Different Categories in Houston and Dallas This faceted chart compares the costs of three different categories‚ÄîMeal for 2, Gasoline (per liter), and 1-bedroom Apartment‚Äîin Houston and Dallas. The chart is split into separate panels for each category, making it easier to compare the cost differences between the two cities.\nThe 1-bedroom Apartment rent is the most expensive category in both cities, with Dallas being slightly higher than Houston. The cost of Gasoline per liter and Meal for 2 is relatively similar in both cities, but still shows minor differences. By breaking the data into different facets, the chart allows for better visibility and comparison between categories that vary greatly in cost. ‚Äî"
  },
  {
    "objectID": "sub4.html#charts-1-4-for-cost-of-living-data-set",
    "href": "sub4.html#charts-1-4-for-cost-of-living-data-set",
    "title": "Assignment 4",
    "section": "",
    "text": "Meal Cost for Two in Houston and Dallas This chart illustrates the cost of a mid-range, three-course meal for two people in Houston, TX and Dallas, TX. The data is based on current cost-of-living figures, highlighting the price differences between these two major cities in Texas.\nHouston has a slightly higher average cost of $53.91 for a meal for two, while Dallas shows a lower cost at $44.92. The width of each bar represents an arbitrary ‚Äúmarket share‚Äù in this chart to demonstrate variable-width column charts, though this could represent the size of the dining market or the popularity of restaurants in these cities. The contrasting colors make it easy to distinguish between the two cities and quickly compare the meal prices. ‚Äî\n\n\n\n\n\n\n\n\n\nMeal Cost Comparison Across Selected US Cities This bar chart compares the cost of a mid-range, three-course meal for two people across four selected cities in the United States: Houston, Dallas, Austin, and Chicago. The data highlights differences in meal pricing across these urban areas:\nHouston has the highest meal cost at approximately $53.91. Dallas follows closely with a meal cost of $44.92. Austin and Chicago both have meal costs around $50, slightly below that of Houston. The custom colors and rotated x-axis labels provide an easy-to-read comparison of meal costs in these cities, helping to visualize regional differences in cost of living. ‚Äî\n\n\n\n\n\n\n\n\n\nCost Comparison for Different Categories in Houston This bar chart compares the costs of three key categories in Houston, Texas: Meal for 2, 1-bedroom Apartment Rent, and Gasoline (per liter). Due to the large disparity in values between these categories, the chart uses a logarithmic scale on the y-axis to provide a clearer comparison:\nThe cost of renting a 1-bedroom apartment in Houston is significantly higher, around $2,311.84. The cost of a mid-range meal for two people is approximately $53.91, much lower in comparison. The price of gasoline per liter is around $1.17. ‚Äî\n\n\n\n\n\n\n\n\n\nCost Comparison for Different Categories in Houston and Dallas This faceted chart compares the costs of three different categories‚ÄîMeal for 2, Gasoline (per liter), and 1-bedroom Apartment‚Äîin Houston and Dallas. The chart is split into separate panels for each category, making it easier to compare the cost differences between the two cities.\nThe 1-bedroom Apartment rent is the most expensive category in both cities, with Dallas being slightly higher than Houston. The cost of Gasoline per liter and Meal for 2 is relatively similar in both cities, but still shows minor differences. By breaking the data into different facets, the chart allows for better visibility and comparison between categories that vary greatly in cost. ‚Äî"
  },
  {
    "objectID": "rev1.html",
    "href": "rev1.html",
    "title": "Edward Tufte‚Äôs The Future of Data Analysis Review",
    "section": "",
    "text": "In the keynote session of the Microsoft Machine Learning & Data Science Summit 2016, Dr.¬†Edward Tufte presented on ‚ÄúThe Future of Data Analysis.‚Äù Dr.¬†Tufte emphasized that data analysis is about ‚Äúturning information into conclusions,‚Äù while analytical thinking involves ‚Äúassessing and evaluating the relationship between information and conclusions.‚Äù The goal of data visualization, according to him, is to ‚Äúassist reasoning about its content.‚Äù This suggests that data visualization should help in illustrating the connections between ‚Äúinformation and conclusions,‚Äù such as demonstrating causal mechanisms or models that aim to describe, explain, and predict observed phenomena.\nOne of the more alarming points Tufte raised was the current crisis in data analysis, highlighting that many published studies are often incorrect. He referred to an article by John P. A. Ioannidis, which claims that over 35 percent of published research findings are false due to factors like study power and bias. Tufte also cited Lazer and colleagues‚Äô (2014) work on the ‚ÄòGoogle Flu‚Äô case to illustrate the dangers of overfitting in big data analysis. Even more concerning is that not only are the original studies‚Äô findings and conclusions often not replicable, but replication studies also fail to achieve consistent results. This calls into question the overall validity of data analyses in empirical research.\nTufte‚Äôs assertion that ‚Äúhuman science is not rocket science; it is harder than rocket science‚Äù is particularly striking. Human behavior and societal dynamics are incredibly complex and ever-changing. This makes data analysis in the social sciences a challenging and evolving field that requires constant refinement and development. So, what is the future of data analysis? According to Tufte, it lies in making a clear distinction between studies that are confirmatory and free from manipulation, and those that are exploratory in nature. He emphasizes that findings should not be manufactured from the data itself but should arise from rigorous and unbiased analysis. Ensuring the quality of data analysis in the social sciences requires us to remain vigilant, judicious, and adaptable. This approach will enable us to generate data analyses and conclusions that are as accurate and reliable as possible."
  },
  {
    "objectID": "rev1.html#review-and-reflection-on-the-video-the-future-of-data-analysis-by-dr.-edward-tufte",
    "href": "rev1.html#review-and-reflection-on-the-video-the-future-of-data-analysis-by-dr.-edward-tufte",
    "title": "Edward Tufte‚Äôs The Future of Data Analysis Review",
    "section": "",
    "text": "In the keynote session of the Microsoft Machine Learning & Data Science Summit 2016, Dr.¬†Edward Tufte presented on ‚ÄúThe Future of Data Analysis.‚Äù Dr.¬†Tufte emphasized that data analysis is about ‚Äúturning information into conclusions,‚Äù while analytical thinking involves ‚Äúassessing and evaluating the relationship between information and conclusions.‚Äù The goal of data visualization, according to him, is to ‚Äúassist reasoning about its content.‚Äù This suggests that data visualization should help in illustrating the connections between ‚Äúinformation and conclusions,‚Äù such as demonstrating causal mechanisms or models that aim to describe, explain, and predict observed phenomena.\nOne of the more alarming points Tufte raised was the current crisis in data analysis, highlighting that many published studies are often incorrect. He referred to an article by John P. A. Ioannidis, which claims that over 35 percent of published research findings are false due to factors like study power and bias. Tufte also cited Lazer and colleagues‚Äô (2014) work on the ‚ÄòGoogle Flu‚Äô case to illustrate the dangers of overfitting in big data analysis. Even more concerning is that not only are the original studies‚Äô findings and conclusions often not replicable, but replication studies also fail to achieve consistent results. This calls into question the overall validity of data analyses in empirical research.\nTufte‚Äôs assertion that ‚Äúhuman science is not rocket science; it is harder than rocket science‚Äù is particularly striking. Human behavior and societal dynamics are incredibly complex and ever-changing. This makes data analysis in the social sciences a challenging and evolving field that requires constant refinement and development. So, what is the future of data analysis? According to Tufte, it lies in making a clear distinction between studies that are confirmatory and free from manipulation, and those that are exploratory in nature. He emphasizes that findings should not be manufactured from the data itself but should arise from rigorous and unbiased analysis. Ensuring the quality of data analysis in the social sciences requires us to remain vigilant, judicious, and adaptable. This approach will enable us to generate data analyses and conclusions that are as accurate and reliable as possible."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Currently a graduate student at The University of Texas at Dallas (UTD). In my last semester in the Social Data Analytics and Research Masters (SDAR) program within the school of Economic, Political, and Policy Sciences (EPPS.)\nWhile completing my graduate education I also work full time as Project Coordinator at Partner Engineering and Science Inc.¬†in the Due Diligence department focusing on Phase I Environmental (ESA) and Property Condition Assessment (PCA) data entry, reports and administrative tasks.\nCombined with my educational and professional experience this in turn has also helped improve my GISC studies and hands on learning as they are applied to my everyday work flow."
  },
  {
    "objectID": "index.html#raquel-luna",
    "href": "index.html#raquel-luna",
    "title": "About",
    "section": "",
    "text": "Currently a graduate student at The University of Texas at Dallas (UTD). In my last semester in the Social Data Analytics and Research Masters (SDAR) program within the school of Economic, Political, and Policy Sciences (EPPS.)\nWhile completing my graduate education I also work full time as Project Coordinator at Partner Engineering and Science Inc.¬†in the Due Diligence department focusing on Phase I Environmental (ESA) and Property Condition Assessment (PCA) data entry, reports and administrative tasks.\nCombined with my educational and professional experience this in turn has also helped improve my GISC studies and hands on learning as they are applied to my everyday work flow."
  },
  {
    "objectID": "index.html#about-this-website",
    "href": "index.html#about-this-website",
    "title": "About",
    "section": "About this Website",
    "text": "About this Website\nHere you will find my coursework assignments for multiple courses done in my graduate studies related to the SDAR program."
  },
  {
    "objectID": "Assignment4.html",
    "href": "Assignment4.html",
    "title": "Assignment 4: Webscraping with rvest",
    "section": "",
    "text": "This assignment uses the rvest package to scrape a table from Wikipedia, cleans the scraped data, and then modifies the code to scrape a second table. I also outline a short data plan for using web data in research.\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(stringr)\nlibrary(lubridate)\n\n\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\n\n\n\n\n\nwikiforreserve &lt;- read_html(url)\nclass(wikiforreserve)\n\n[1] \"xml_document\" \"xml_node\"    \n\n\n\n\n\n\n## At Inspect tab, look for\ntag. Leave the table close ## Right click the table and Copy XPath, paste at html_nodes(xpath =)\nforeignreserve &lt;- wikiforreserve %&gt;%\n  html_nodes(xpath='//*[@id=\"mw-content-text\"]/div/table[1]') %&gt;%\n  html_table()\n\nfores &lt;- foreignreserve[[1]]\n\ncolnames(fores)\n\n[1] \"Country(as recognized by the U.N.)\" \"Continent\"                         \n[3] \"Foreign exchange reserves\"          \"Foreign exchange reserves\"         \n[5] \"Foreign exchange reserves\"          \"Foreign exchange reserves\"         \n[7] \"Last reporteddate\"                  \"Ref.\"                              \n\n\nnames(fores)[1:length(names(fores))] &lt;- c(\n  \"Country\",            \n  \"Continent\",          \n  \"Incl_gold_USD\",      \n  \"Incl_gold_change\",   \n  \"Excl_gold_USD\",      \n  \"Excl_gold_change\",   \n  \"Last_reported\",      \n  \"Ref\"                 \n)[seq_along(names(fores))]\n\ncolnames(fores)\n\n[1] \"Country\"          \"Continent\"        \"Incl_gold_USD\"    \"Incl_gold_change\"\n[5] \"Excl_gold_USD\"    \"Excl_gold_change\" \"Last_reported\"    \"Ref\"             \n\nhead(fores, 5)\n\n# A tibble: 5 √ó 8\n  Country                 Continent Incl_gold_USD Incl_gold_change Excl_gold_USD\n  &lt;chr&gt;                   &lt;chr&gt;     &lt;chr&gt;         &lt;chr&gt;            &lt;chr&gt;        \n1 Country(as recognized ‚Ä¶ Continent Including go‚Ä¶ Including gold   Excluding go‚Ä¶\n2 Country(as recognized ‚Ä¶ Continent millions U.S‚Ä¶ Change           millions U.S‚Ä¶\n3 China                   Asia      3,643,149     41,079           3,389,306    \n4 Japan                   Asia      1,324,210     19,774           1,230,940    \n5 Switzerland             Europe    1,007,710     13,935           897,295      \n# ‚Ñπ 3 more variables: Excl_gold_change &lt;chr&gt;, Last_reported &lt;chr&gt;, Ref &lt;chr&gt;\n\n\n\n\nlibrary(readr)\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(stringr)\n\nfores_clean &lt;- fores %&gt;%\n  filter(!str_detect(Country, \"Country\\\\(as recognized by the U\\\\.N\\\\.\\\\)\")) %&gt;%\n  mutate(\n    Incl_gold_USD_num    = parse_number(Incl_gold_USD),\n    Incl_gold_change_num = parse_number(Incl_gold_change),\n    Excl_gold_USD_num    = parse_number(Excl_gold_USD),\n    Excl_gold_change_num = parse_number(Excl_gold_change),\n    Last_reported_clean  = str_trim(Last_reported),\n    Last_reported_date   = suppressWarnings(dmy(Last_reported_clean))\n  ) %&gt;%\n  select(\n    Country,\n    Continent,\n    Incl_gold_USD_num,\n    Excl_gold_USD_num,\n    Incl_gold_change_num,\n    Excl_gold_change_num,\n    Last_reported_date\n  )\n\nfores_clean %&gt;% head(10)\n\n# A tibble: 10 √ó 7\n   Country    Continent Incl_gold_USD_num Excl_gold_USD_num Incl_gold_change_num\n   &lt;chr&gt;      &lt;chr&gt;                 &lt;dbl&gt;             &lt;dbl&gt;                &lt;dbl&gt;\n 1 China      Asia                3643149           3389306                41079\n 2 Japan      Asia                1324210           1230940                19774\n 3 Switzerla‚Ä¶ Europe              1007710            897295                13935\n 4 Russia     Europe/A‚Ä¶            734100            434487                14300\n 5 India      Asia                 692576            585719                 5543\n 6 Taiwan     Asia                 597430            544300                 4390\n 7 Saudi Ara‚Ä¶ Asia                 434547            434116                21728\n 8 Hong Kong  Asia                 421400            416216                 5126\n 9 South Kor‚Ä¶ Asia                 415700            410900                 4300\n10 Brazil     Americas             388571            344173                 7465\n# ‚Ñπ 2 more variables: Excl_gold_change_num &lt;dbl&gt;, Last_reported_date &lt;date&gt;\n\n\n\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(stringr)\n\ngdp_url &lt;- \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n\ngdp_html &lt;- read_html(gdp_url)\n\ngdp_tables &lt;- gdp_html %&gt;%\n  html_table(fill = TRUE)\n\nlength(gdp_tables)        \n\n[1] 7\n\ngdp_raw &lt;- gdp_tables[[1]]\n\ncolnames(gdp_raw)\n\n[1] \"X1\"\n\nhead(gdp_raw, 5)\n\n# A tibble: 2 √ó 1\n  X1                                                                            \n  &lt;chr&gt;                                                                         \n1 \"\"                                                                            \n2 \"Largest economies in the world by GDP (nominal) in 2025according to Internat‚Ä¶\n\n\n\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(stringr)\n\ngdp_url &lt;- \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n\ngdp_html &lt;- read_html(gdp_url)\n\ngdp_raw &lt;- gdp_html %&gt;%\n  html_element(\"table.wikitable\") %&gt;% \n  html_table(fill = TRUE)\n\ncolnames(gdp_raw)\n\n[1] \"Country/Territory\"       \"IMF(2025)[6]\"           \n[3] \"World Bank(2024)[7]\"     \"United Nations(2023)[8]\"\n\nhead(gdp_raw, 5)\n\n# A tibble: 5 √ó 4\n  `Country/Territory` `IMF(2025)[6]` `World Bank(2024)[7]`\n  &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                \n1 World               117,165,394    111,326,370          \n2 United States       30,615,743     29,184,890           \n3 China[n 1]          19,398,577     18,743,803           \n4 Germany             5,013,574      4,659,929            \n5 Japan               4,279,828      4,026,211            \n# ‚Ñπ 1 more variable: `United Nations(2023)[8]` &lt;chr&gt;\n\n\n\ngdp_raw &lt;- gdp_raw %&gt;%\n  rename(\n    Country         = 1,\n    IMF_2025        = 2,\n    WorldBank_2024  = 3,\n    UN_2023         = 4\n  )\ngdp_clean &lt;- gdp_raw %&gt;%\n  mutate(\n    Country = str_trim(gsub(\"\\\\[.*?\\\\]\", \"\", Country)),\n    IMF_2025       = parse_number(IMF_2025),\n    WorldBank_2024 = parse_number(WorldBank_2024),\n    UN_2023        = parse_number(UN_2023)\n  ) %&gt;%\n  filter(Country != \"World\")\ngdp_clean %&gt;% head(10)\n\n# A tibble: 10 √ó 4\n   Country        IMF_2025 WorldBank_2024  UN_2023\n   &lt;chr&gt;             &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n 1 United States  30615743       29184890 27720700\n 2 China          19398577       18743803 17794782\n 3 Germany         5013574        4659929  4525704\n 4 Japan           4279828        4026211  4204495\n 5 India           4125213        3912686  3575778\n 6 United Kingdom  3958780        3643834  3380855\n 7 France          3361557        3162079  3051832\n 8 Italy           2543677        2372775  2300941\n 9 Russia          2540656        2173836  2008419\n10 Canada          2283599        2241253  2142471\n\n\n\n\nFor future research, I would first choose a few trustworthy websites that regularly post the information I need, such as government sites or pages from organizations like the IMF or World Bank. Then I would use R tools like rvest or APIs (if the website offers them) to collect the data automatically. After pulling the data, I would clean it, check that the values make sense, and save everything in a consistent format. I would also set up a script to update the data on a regular schedule so I can track changes over time without having to scrape everything manually."
  },
  {
    "objectID": "Assignment4.html#part-a-scraping-foreign-reserve-data-from-wikipedia",
    "href": "Assignment4.html#part-a-scraping-foreign-reserve-data-from-wikipedia",
    "title": "Assignment 4: Webscraping with rvest",
    "section": "",
    "text": "library(tidyverse)\nlibrary(rvest)\nlibrary(stringr)\nlibrary(lubridate)\n\n\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\n\n\n\n\n\nwikiforreserve &lt;- read_html(url)\nclass(wikiforreserve)\n\n[1] \"xml_document\" \"xml_node\""
  },
  {
    "objectID": "Assignment4.html#get-the-xpath-data-using-inspect-element-feature-in-safari-chrome-or-firefox",
    "href": "Assignment4.html#get-the-xpath-data-using-inspect-element-feature-in-safari-chrome-or-firefox",
    "title": "Assignment 4: Webscraping with rvest",
    "section": "",
    "text": "## At Inspect tab, look for\ntag. Leave the table close ## Right click the table and Copy XPath, paste at html_nodes(xpath =)\nforeignreserve &lt;- wikiforreserve %&gt;%\n  html_nodes(xpath='//*[@id=\"mw-content-text\"]/div/table[1]') %&gt;%\n  html_table()\n\nfores &lt;- foreignreserve[[1]]\n\ncolnames(fores)\n\n[1] \"Country(as recognized by the U.N.)\" \"Continent\"                         \n[3] \"Foreign exchange reserves\"          \"Foreign exchange reserves\"         \n[5] \"Foreign exchange reserves\"          \"Foreign exchange reserves\"         \n[7] \"Last reporteddate\"                  \"Ref.\"                              \n\n\nnames(fores)[1:length(names(fores))] &lt;- c(\n  \"Country\",            \n  \"Continent\",          \n  \"Incl_gold_USD\",      \n  \"Incl_gold_change\",   \n  \"Excl_gold_USD\",      \n  \"Excl_gold_change\",   \n  \"Last_reported\",      \n  \"Ref\"                 \n)[seq_along(names(fores))]\n\ncolnames(fores)\n\n[1] \"Country\"          \"Continent\"        \"Incl_gold_USD\"    \"Incl_gold_change\"\n[5] \"Excl_gold_USD\"    \"Excl_gold_change\" \"Last_reported\"    \"Ref\"             \n\nhead(fores, 5)\n\n# A tibble: 5 √ó 8\n  Country                 Continent Incl_gold_USD Incl_gold_change Excl_gold_USD\n  &lt;chr&gt;                   &lt;chr&gt;     &lt;chr&gt;         &lt;chr&gt;            &lt;chr&gt;        \n1 Country(as recognized ‚Ä¶ Continent Including go‚Ä¶ Including gold   Excluding go‚Ä¶\n2 Country(as recognized ‚Ä¶ Continent millions U.S‚Ä¶ Change           millions U.S‚Ä¶\n3 China                   Asia      3,643,149     41,079           3,389,306    \n4 Japan                   Asia      1,324,210     19,774           1,230,940    \n5 Switzerland             Europe    1,007,710     13,935           897,295      \n# ‚Ñπ 3 more variables: Excl_gold_change &lt;chr&gt;, Last_reported &lt;chr&gt;, Ref &lt;chr&gt;\n\n\n\n\nlibrary(readr)\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(stringr)\n\nfores_clean &lt;- fores %&gt;%\n  filter(!str_detect(Country, \"Country\\\\(as recognized by the U\\\\.N\\\\.\\\\)\")) %&gt;%\n  mutate(\n    Incl_gold_USD_num    = parse_number(Incl_gold_USD),\n    Incl_gold_change_num = parse_number(Incl_gold_change),\n    Excl_gold_USD_num    = parse_number(Excl_gold_USD),\n    Excl_gold_change_num = parse_number(Excl_gold_change),\n    Last_reported_clean  = str_trim(Last_reported),\n    Last_reported_date   = suppressWarnings(dmy(Last_reported_clean))\n  ) %&gt;%\n  select(\n    Country,\n    Continent,\n    Incl_gold_USD_num,\n    Excl_gold_USD_num,\n    Incl_gold_change_num,\n    Excl_gold_change_num,\n    Last_reported_date\n  )\n\nfores_clean %&gt;% head(10)\n\n# A tibble: 10 √ó 7\n   Country    Continent Incl_gold_USD_num Excl_gold_USD_num Incl_gold_change_num\n   &lt;chr&gt;      &lt;chr&gt;                 &lt;dbl&gt;             &lt;dbl&gt;                &lt;dbl&gt;\n 1 China      Asia                3643149           3389306                41079\n 2 Japan      Asia                1324210           1230940                19774\n 3 Switzerla‚Ä¶ Europe              1007710            897295                13935\n 4 Russia     Europe/A‚Ä¶            734100            434487                14300\n 5 India      Asia                 692576            585719                 5543\n 6 Taiwan     Asia                 597430            544300                 4390\n 7 Saudi Ara‚Ä¶ Asia                 434547            434116                21728\n 8 Hong Kong  Asia                 421400            416216                 5126\n 9 South Kor‚Ä¶ Asia                 415700            410900                 4300\n10 Brazil     Americas             388571            344173                 7465\n# ‚Ñπ 2 more variables: Excl_gold_change_num &lt;dbl&gt;, Last_reported_date &lt;date&gt;\n\n\n\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(stringr)\n\ngdp_url &lt;- \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n\ngdp_html &lt;- read_html(gdp_url)\n\ngdp_tables &lt;- gdp_html %&gt;%\n  html_table(fill = TRUE)\n\nlength(gdp_tables)        \n\n[1] 7\n\ngdp_raw &lt;- gdp_tables[[1]]\n\ncolnames(gdp_raw)\n\n[1] \"X1\"\n\nhead(gdp_raw, 5)\n\n# A tibble: 2 √ó 1\n  X1                                                                            \n  &lt;chr&gt;                                                                         \n1 \"\"                                                                            \n2 \"Largest economies in the world by GDP (nominal) in 2025according to Internat‚Ä¶\n\n\n\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(stringr)\n\ngdp_url &lt;- \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n\ngdp_html &lt;- read_html(gdp_url)\n\ngdp_raw &lt;- gdp_html %&gt;%\n  html_element(\"table.wikitable\") %&gt;% \n  html_table(fill = TRUE)\n\ncolnames(gdp_raw)\n\n[1] \"Country/Territory\"       \"IMF(2025)[6]\"           \n[3] \"World Bank(2024)[7]\"     \"United Nations(2023)[8]\"\n\nhead(gdp_raw, 5)\n\n# A tibble: 5 √ó 4\n  `Country/Territory` `IMF(2025)[6]` `World Bank(2024)[7]`\n  &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                \n1 World               117,165,394    111,326,370          \n2 United States       30,615,743     29,184,890           \n3 China[n 1]          19,398,577     18,743,803           \n4 Germany             5,013,574      4,659,929            \n5 Japan               4,279,828      4,026,211            \n# ‚Ñπ 1 more variable: `United Nations(2023)[8]` &lt;chr&gt;\n\n\n\ngdp_raw &lt;- gdp_raw %&gt;%\n  rename(\n    Country         = 1,\n    IMF_2025        = 2,\n    WorldBank_2024  = 3,\n    UN_2023         = 4\n  )\ngdp_clean &lt;- gdp_raw %&gt;%\n  mutate(\n    Country = str_trim(gsub(\"\\\\[.*?\\\\]\", \"\", Country)),\n    IMF_2025       = parse_number(IMF_2025),\n    WorldBank_2024 = parse_number(WorldBank_2024),\n    UN_2023        = parse_number(UN_2023)\n  ) %&gt;%\n  filter(Country != \"World\")\ngdp_clean %&gt;% head(10)\n\n# A tibble: 10 √ó 4\n   Country        IMF_2025 WorldBank_2024  UN_2023\n   &lt;chr&gt;             &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n 1 United States  30615743       29184890 27720700\n 2 China          19398577       18743803 17794782\n 3 Germany         5013574        4659929  4525704\n 4 Japan           4279828        4026211  4204495\n 5 India           4125213        3912686  3575778\n 6 United Kingdom  3958780        3643834  3380855\n 7 France          3361557        3162079  3051832\n 8 Italy           2543677        2372775  2300941\n 9 Russia          2540656        2173836  2008419\n10 Canada          2283599        2241253  2142471\n\n\n\n\nFor future research, I would first choose a few trustworthy websites that regularly post the information I need, such as government sites or pages from organizations like the IMF or World Bank. Then I would use R tools like rvest or APIs (if the website offers them) to collect the data automatically. After pulling the data, I would clean it, check that the values make sense, and save everything in a consistent format. I would also set up a script to update the data on a regular schedule so I can track changes over time without having to scrape everything manually."
  },
  {
    "objectID": "Assignment4.html#clean-up-variables",
    "href": "Assignment4.html#clean-up-variables",
    "title": "Assignment 4: Webscraping with rvest",
    "section": "",
    "text": "library(readr)\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(stringr)\n\nfores_clean &lt;- fores %&gt;%\n  filter(!str_detect(Country, \"Country\\\\(as recognized by the U\\\\.N\\\\.\\\\)\")) %&gt;%\n  mutate(\n    Incl_gold_USD_num    = parse_number(Incl_gold_USD),\n    Incl_gold_change_num = parse_number(Incl_gold_change),\n    Excl_gold_USD_num    = parse_number(Excl_gold_USD),\n    Excl_gold_change_num = parse_number(Excl_gold_change),\n    Last_reported_clean  = str_trim(Last_reported),\n    Last_reported_date   = suppressWarnings(dmy(Last_reported_clean))\n  ) %&gt;%\n  select(\n    Country,\n    Continent,\n    Incl_gold_USD_num,\n    Excl_gold_USD_num,\n    Incl_gold_change_num,\n    Excl_gold_change_num,\n    Last_reported_date\n  )\n\nfores_clean %&gt;% head(10)\n\n# A tibble: 10 √ó 7\n   Country    Continent Incl_gold_USD_num Excl_gold_USD_num Incl_gold_change_num\n   &lt;chr&gt;      &lt;chr&gt;                 &lt;dbl&gt;             &lt;dbl&gt;                &lt;dbl&gt;\n 1 China      Asia                3643149           3389306                41079\n 2 Japan      Asia                1324210           1230940                19774\n 3 Switzerla‚Ä¶ Europe              1007710            897295                13935\n 4 Russia     Europe/A‚Ä¶            734100            434487                14300\n 5 India      Asia                 692576            585719                 5543\n 6 Taiwan     Asia                 597430            544300                 4390\n 7 Saudi Ara‚Ä¶ Asia                 434547            434116                21728\n 8 Hong Kong  Asia                 421400            416216                 5126\n 9 South Kor‚Ä¶ Asia                 415700            410900                 4300\n10 Brazil     Americas             388571            344173                 7465\n# ‚Ñπ 2 more variables: Excl_gold_change_num &lt;dbl&gt;, Last_reported_date &lt;date&gt;\n\n\n\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(stringr)\n\ngdp_url &lt;- \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n\ngdp_html &lt;- read_html(gdp_url)\n\ngdp_tables &lt;- gdp_html %&gt;%\n  html_table(fill = TRUE)\n\nlength(gdp_tables)        \n\n[1] 7\n\ngdp_raw &lt;- gdp_tables[[1]]\n\ncolnames(gdp_raw)\n\n[1] \"X1\"\n\nhead(gdp_raw, 5)\n\n# A tibble: 2 √ó 1\n  X1                                                                            \n  &lt;chr&gt;                                                                         \n1 \"\"                                                                            \n2 \"Largest economies in the world by GDP (nominal) in 2025according to Internat‚Ä¶\n\n\n\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(stringr)\n\ngdp_url &lt;- \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n\ngdp_html &lt;- read_html(gdp_url)\n\ngdp_raw &lt;- gdp_html %&gt;%\n  html_element(\"table.wikitable\") %&gt;% \n  html_table(fill = TRUE)\n\ncolnames(gdp_raw)\n\n[1] \"Country/Territory\"       \"IMF(2025)[6]\"           \n[3] \"World Bank(2024)[7]\"     \"United Nations(2023)[8]\"\n\nhead(gdp_raw, 5)\n\n# A tibble: 5 √ó 4\n  `Country/Territory` `IMF(2025)[6]` `World Bank(2024)[7]`\n  &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                \n1 World               117,165,394    111,326,370          \n2 United States       30,615,743     29,184,890           \n3 China[n 1]          19,398,577     18,743,803           \n4 Germany             5,013,574      4,659,929            \n5 Japan               4,279,828      4,026,211            \n# ‚Ñπ 1 more variable: `United Nations(2023)[8]` &lt;chr&gt;\n\n\n\ngdp_raw &lt;- gdp_raw %&gt;%\n  rename(\n    Country         = 1,\n    IMF_2025        = 2,\n    WorldBank_2024  = 3,\n    UN_2023         = 4\n  )\ngdp_clean &lt;- gdp_raw %&gt;%\n  mutate(\n    Country = str_trim(gsub(\"\\\\[.*?\\\\]\", \"\", Country)),\n    IMF_2025       = parse_number(IMF_2025),\n    WorldBank_2024 = parse_number(WorldBank_2024),\n    UN_2023        = parse_number(UN_2023)\n  ) %&gt;%\n  filter(Country != \"World\")\ngdp_clean %&gt;% head(10)\n\n# A tibble: 10 √ó 4\n   Country        IMF_2025 WorldBank_2024  UN_2023\n   &lt;chr&gt;             &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n 1 United States  30615743       29184890 27720700\n 2 China          19398577       18743803 17794782\n 3 Germany         5013574        4659929  4525704\n 4 Japan           4279828        4026211  4204495\n 5 India           4125213        3912686  3575778\n 6 United Kingdom  3958780        3643834  3380855\n 7 France          3361557        3162079  3051832\n 8 Italy           2543677        2372775  2300941\n 9 Russia          2540656        2173836  2008419\n10 Canada          2283599        2241253  2142471"
  },
  {
    "objectID": "Assignment4.html#data-plan-for-getting-web-data",
    "href": "Assignment4.html#data-plan-for-getting-web-data",
    "title": "Assignment 4: Webscraping with rvest",
    "section": "",
    "text": "For future research, I would first choose a few trustworthy websites that regularly post the information I need, such as government sites or pages from organizations like the IMF or World Bank. Then I would use R tools like rvest or APIs (if the website offers them) to collect the data automatically. After pulling the data, I would clean it, check that the values make sense, and save everything in a consistent format. I would also set up a script to update the data on a regular schedule so I can track changes over time without having to scrape everything manually."
  },
  {
    "objectID": "Assignment2.html",
    "href": "Assignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "This assignment compares manually downloaded Google Trends data with data collected using the gtrendsR package. The goal is to analyze the dates and intervals in the manual dataset, save both datasets, and describe methodological differences.\n\n\n\nlibrary(tidyverse)\nmanual_raw &lt;- read_csv(\"multiTimeline.csv\", skip = 1)\nmanual &lt;- manual_raw %&gt;%\n  rename(\n    Time = Time,\n    Trump = `Trump: (United States)`,\n    Kamala_Harris_Election = `Kamala Harris and Election: (United States)`\n  ) %&gt;%\n  mutate(\n    Time = as.POSIXct(Time),\n    Kamala_Harris_Election = na_if(Kamala_Harris_Election, \"&lt;1\"),\n    Kamala_Harris_Election = as.numeric(Kamala_Harris_Election)\n  )\nmanual %&gt;% head()\n\n# A tibble: 6 √ó 3\n  Time                Trump Kamala_Harris_Election\n  &lt;dttm&gt;              &lt;dbl&gt;                  &lt;dbl&gt;\n1 2025-11-23 03:44:00    85                      0\n2 2025-11-23 03:52:00    81                      0\n3 2025-11-23 04:00:00    78                      0\n4 2025-11-23 04:08:00    81                      0\n5 2025-11-23 04:16:00    78                      0\n6 2025-11-23 04:24:00    78                      0\n\n\n\n\n\nrange(manual$Time)\n\n[1] \"2025-11-23 03:44:00 UTC\" \"2025-11-24 03:44:00 UTC\"\n\n\n\n\n\n\nsummary(diff(manual$Time))\n\n  Length    Class     Mode \n     180 difftime  numeric \n\n\n\n\n\n\n\nlibrary(gtrendsR)\nTrumpHarrisElection &lt;- gtrends(\n  c(\"Trump\", \"Harris\", \"election\"),\n  onlyInterest = TRUE,\n  geo = \"US\",\n  gprop = \"web\",\n  time = \"today+5-y\"\n)\ngtr_ts &lt;- TrumpHarrisElection$interest_over_time\nhead(gtr_ts)\n\n        date hits keyword geo      time gprop category\n1 2020-11-29    5   Trump  US today+5-y   web        0\n2 2020-12-06    5   Trump  US today+5-y   web        0\n3 2020-12-13    5   Trump  US today+5-y   web        0\n4 2020-12-20    6   Trump  US today+5-y   web        0\n5 2020-12-27    5   Trump  US today+5-y   web        0\n6 2021-01-03   15   Trump  US today+5-y   web        0\n\n\n\n\n\n\nwrite_csv(manual, \"manual_trends.csv\")\nsaveRDS(manual, \"manual_trends.rds\")\nwrite_csv(gtr_ts, \"gtrends_trends.csv\")\nsaveRDS(gtr_ts, \"gtrends_trends.rds\")\n\n\n\n\nThe manual CSV only covers a short, recent period of time with frequent timestamps.The gtrendsR call returns a longer time series over the last five years. The intervals in the gtrendsR output are usually weekly instead of minute-by-minute. The manual file is wide, with separate columns for each search term. The gtrendsR output is long, with one column indicating the term (keyword) and another column for the search interest (hits). Both data sets report relative search interest on a 0‚Äì100 scale, but the values are not the same. The manual download is normalized for the specific time window selected on the website, and the gtrendsR results are normalized over the longer five-year period."
  },
  {
    "objectID": "Assignment2.html#part-a-collecting-data-using-google-trends",
    "href": "Assignment2.html#part-a-collecting-data-using-google-trends",
    "title": "Assignment 2",
    "section": "",
    "text": "library(tidyverse)\nmanual_raw &lt;- read_csv(\"multiTimeline.csv\", skip = 1)\nmanual &lt;- manual_raw %&gt;%\n  rename(\n    Time = Time,\n    Trump = `Trump: (United States)`,\n    Kamala_Harris_Election = `Kamala Harris and Election: (United States)`\n  ) %&gt;%\n  mutate(\n    Time = as.POSIXct(Time),\n    Kamala_Harris_Election = na_if(Kamala_Harris_Election, \"&lt;1\"),\n    Kamala_Harris_Election = as.numeric(Kamala_Harris_Election)\n  )\nmanual %&gt;% head()\n\n# A tibble: 6 √ó 3\n  Time                Trump Kamala_Harris_Election\n  &lt;dttm&gt;              &lt;dbl&gt;                  &lt;dbl&gt;\n1 2025-11-23 03:44:00    85                      0\n2 2025-11-23 03:52:00    81                      0\n3 2025-11-23 04:00:00    78                      0\n4 2025-11-23 04:08:00    81                      0\n5 2025-11-23 04:16:00    78                      0\n6 2025-11-23 04:24:00    78                      0\n\n\n\n\n\nrange(manual$Time)\n\n[1] \"2025-11-23 03:44:00 UTC\" \"2025-11-24 03:44:00 UTC\"\n\n\n\n\n\n\nsummary(diff(manual$Time))\n\n  Length    Class     Mode \n     180 difftime  numeric"
  },
  {
    "objectID": "Assignment2.html#part-b-collecting-data-using-gtrendsr",
    "href": "Assignment2.html#part-b-collecting-data-using-gtrendsr",
    "title": "Assignment 2",
    "section": "",
    "text": "library(gtrendsR)\nTrumpHarrisElection &lt;- gtrends(\n  c(\"Trump\", \"Harris\", \"election\"),\n  onlyInterest = TRUE,\n  geo = \"US\",\n  gprop = \"web\",\n  time = \"today+5-y\"\n)\ngtr_ts &lt;- TrumpHarrisElection$interest_over_time\nhead(gtr_ts)\n\n        date hits keyword geo      time gprop category\n1 2020-11-29    5   Trump  US today+5-y   web        0\n2 2020-12-06    5   Trump  US today+5-y   web        0\n3 2020-12-13    5   Trump  US today+5-y   web        0\n4 2020-12-20    6   Trump  US today+5-y   web        0\n5 2020-12-27    5   Trump  US today+5-y   web        0\n6 2021-01-03   15   Trump  US today+5-y   web        0"
  },
  {
    "objectID": "Assignment2.html#part-c-saving-the-data",
    "href": "Assignment2.html#part-c-saving-the-data",
    "title": "Assignment 2",
    "section": "",
    "text": "write_csv(manual, \"manual_trends.csv\")\nsaveRDS(manual, \"manual_trends.rds\")\nwrite_csv(gtr_ts, \"gtrends_trends.csv\")\nsaveRDS(gtr_ts, \"gtrends_trends.rds\")"
  },
  {
    "objectID": "Assignment2.html#part-d-method-comparison",
    "href": "Assignment2.html#part-d-method-comparison",
    "title": "Assignment 2",
    "section": "",
    "text": "The manual CSV only covers a short, recent period of time with frequent timestamps.The gtrendsR call returns a longer time series over the last five years. The intervals in the gtrendsR output are usually weekly instead of minute-by-minute. The manual file is wide, with separate columns for each search term. The gtrendsR output is long, with one column indicating the term (keyword) and another column for the search interest (hits). Both data sets report relative search interest on a 0‚Äì100 scale, but the values are not the same. The manual download is normalized for the specific time window selected on the website, and the gtrendsR results are normalized over the longer five-year period."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Currently a graduate student at The University of Texas at Dallas (UTD). In my last semester in the Social Data Analytics and Research Masters (SDAR) program within the school of Economic, Political, and Policy Sciences (EPPS.)\nWhile completing my graduate education I also work full time as Project Coordinator at Partner Engineering and Science Inc.¬†in the Due Diligence department focusing on Phase I Environmental (ESA) and Property Condition Assessment (PCA) data entry, reports and administrative tasks.\nCombined with my educational and professional experience this in turn has also helped improve my GISC studies and hands on learning as they are applied to my everyday work flow."
  },
  {
    "objectID": "about.html#raquel-luna",
    "href": "about.html#raquel-luna",
    "title": "About",
    "section": "",
    "text": "Currently a graduate student at The University of Texas at Dallas (UTD). In my last semester in the Social Data Analytics and Research Masters (SDAR) program within the school of Economic, Political, and Policy Sciences (EPPS.)\nWhile completing my graduate education I also work full time as Project Coordinator at Partner Engineering and Science Inc.¬†in the Due Diligence department focusing on Phase I Environmental (ESA) and Property Condition Assessment (PCA) data entry, reports and administrative tasks.\nCombined with my educational and professional experience this in turn has also helped improve my GISC studies and hands on learning as they are applied to my everyday work flow."
  },
  {
    "objectID": "about.html#about-this-website",
    "href": "about.html#about-this-website",
    "title": "About",
    "section": "About this Website",
    "text": "About this Website\nHere you will find my coursework assignments for multiple courses done in my graduate studies related to the SDAR program."
  },
  {
    "objectID": "Assignment1.html",
    "href": "Assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Website Design\nThis website was created using Rstudio Quarto as part of my coursework for Methods of Data Collection and Production in Dr.¬†Karl Ho‚Äôs class.\n\n\nNavigation Bar\nThe navigation bar provides a users ability to easily access different areas on my site.\n\nHome - is the landing page where the visitor can learn a little bit about my educational and professional background.\nData Visualization - provides assignemtns I completed in one of my graduate courses.\nMethods of Data COllection & Production - houses the coursework for my current class.\nResume - Provides acces to my professional resume , with a download button for convenience.\n\n\n\nCustomization\nMy site was designed with Images, Embedded PDFs where applicable and download buttons for convenience.\n\n\nSummary\nMy website design balances professionalismm easy navigation, as well as my own preference for a welcoming light blue color for a touch of personality."
  },
  {
    "objectID": "Assignment3.html",
    "href": "Assignment3.html",
    "title": "Assignment3",
    "section": "",
    "text": "This assignment retrieves ACS 2023 5-year estimates from the U.S. Census Bureau using the tidycensus package. I selected Texas counties as the geography and used two ACS variables: median household income and poverty count. A choropleth map visualizes income, while a table lists the counties with the highest and lowest poverty counts.\n\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\n\noptions(tigris_use_cache = TRUE)\n\n\n\n\ncensus_api_key(\"326daf02214d3d3502dd97b6d473e77a82baf1c2\", install = FALSE)\n\n\n\n\n\nvars &lt;- load_variables(2023, \"acs5\", cache = TRUE)\n\n\n\n\n\nvars |&gt; dplyr::filter(grepl(\"^B19\", name)) |&gt; dplyr::slice_head(n = 10)\n\n# A tibble: 10 √ó 4\n   name        label                                concept            geography\n   &lt;chr&gt;       &lt;chr&gt;                                &lt;chr&gt;              &lt;chr&gt;    \n 1 B19001A_001 Estimate!!Total:                     Household Income ‚Ä¶ tract    \n 2 B19001A_002 Estimate!!Total:!!Less than $10,000  Household Income ‚Ä¶ tract    \n 3 B19001A_003 Estimate!!Total:!!$10,000 to $14,999 Household Income ‚Ä¶ tract    \n 4 B19001A_004 Estimate!!Total:!!$15,000 to $19,999 Household Income ‚Ä¶ tract    \n 5 B19001A_005 Estimate!!Total:!!$20,000 to $24,999 Household Income ‚Ä¶ tract    \n 6 B19001A_006 Estimate!!Total:!!$25,000 to $29,999 Household Income ‚Ä¶ tract    \n 7 B19001A_007 Estimate!!Total:!!$30,000 to $34,999 Household Income ‚Ä¶ tract    \n 8 B19001A_008 Estimate!!Total:!!$35,000 to $39,999 Household Income ‚Ä¶ tract    \n 9 B19001A_009 Estimate!!Total:!!$40,000 to $44,999 Household Income ‚Ä¶ tract    \n10 B19001A_010 Estimate!!Total:!!$45,000 to $49,999 Household Income ‚Ä¶ tract    \n\n\n\n\n\n\nstate_abbr &lt;- \"TX\"\ngeo_level  &lt;- \"county\"   # options: state, county, tract, block group\nmy_vars &lt;- c(home_value = \"B25077_001\", unemployment = \"B23025_005\")\nyear_acs   &lt;- 2023\nsurvey     &lt;- \"acs5\"\n\n\n\n\n\nacs &lt;- get_acs(\n  geography = geo_level,\n  variables = my_vars,\n  state = state_abbr,\n  year = year_acs,\n  survey = survey,\n  geometry = TRUE\n)\n\n\n\n\n\nacs_wide &lt;- acs |&gt;\n  tidyr::pivot_wider(\n    id_cols = c(GEOID, NAME, geometry),\n    names_from = variable,\n    values_from = c(estimate, moe)\n  )\n\n\n\n\n\nggplot(acs_wide) +\n  geom_sf(aes(fill = estimate_home_value), color = NA) +\n  scale_fill_viridis_c(name = \"Median Home Value\") +\n  labs(title = paste0(\"ACS \", year_acs, \" 5-year: Median Home Value ‚Äì \", state_abbr, \" (\", geo_level, \")\"),\n       caption = \"Source: U.S. Census Bureau via tidycensus\") +\n  theme_minimal() +\n  theme(text=element_text(family = \"Palatino\"),\n        plot.title = element_text(size = 14, face = \"bold\"),\n        plot.caption = element_text(size = 8), legend.position = \"inside\",\n        legend.position.inside = c(0.2, 0.15)) # move legend inside the chart\n\n\n\n\n\n\n\n\n\n\n\n\ntop10 &lt;- acs_wide |&gt;\n  arrange(desc(estimate_unemployment)) |&gt;\n  select(NAME, estimate_unemployment, moe_unemployment) |&gt;\n  slice_head(n = 10)\n\nbottom10 &lt;- acs_wide |&gt;\n  arrange(estimate_unemployment) |&gt;\n  select(NAME, estimate_unemployment, moe_unemployment) |&gt;\n  slice_head(n = 10)\n\ntop10\n\nSimple feature collection with 10 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -106.6456 ymin: 26.03642 xmax: -94.9085 ymax: 33.43045\nGeodetic CRS:  NAD83\n# A tibble: 10 √ó 4\n   NAME         estimate_unemployment moe_unemployment                  geometry\n   &lt;chr&gt;                        &lt;dbl&gt;            &lt;dbl&gt;        &lt;MULTIPOLYGON [¬∞]&gt;\n 1 Harris Coun‚Ä¶                158268             4767 (((-94.97839 29.68365, -‚Ä¶\n 2 Dallas Coun‚Ä¶                 67955             2797 (((-97.03852 32.56, -97.‚Ä¶\n 3 Bexar Count‚Ä¶                 56081             2503 (((-98.80655 29.69071, -‚Ä¶\n 4 Tarrant Cou‚Ä¶                 53014             2568 (((-97.55053 32.56258, -‚Ä¶\n 5 Travis Coun‚Ä¶                 33873             2448 (((-98.15927 30.37665, -‚Ä¶\n 6 Hidalgo Cou‚Ä¶                 28197             1871 (((-98.58634 26.25824, -‚Ä¶\n 7 Collin Coun‚Ä¶                 25247             1528 (((-96.8441 32.98891, -9‚Ä¶\n 8 El Paso Cou‚Ä¶                 24609             1658 (((-106.6455 31.89867, -‚Ä¶\n 9 Fort Bend C‚Ä¶                 21887             1565 (((-96.08891 29.60166, -‚Ä¶\n10 Denton Coun‚Ä¶                 21327             1762 (((-97.39826 32.99996, -‚Ä¶\n\nbottom10\n\nSimple feature collection with 10 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -104.98 ymin: 26.598 xmax: -97.29015 ymax: 35.18326\nGeodetic CRS:  NAD83\n# A tibble: 10 √ó 4\n   NAME         estimate_unemployment moe_unemployment                  geometry\n   &lt;chr&gt;                        &lt;dbl&gt;            &lt;dbl&gt;        &lt;MULTIPOLYGON [¬∞]&gt;\n 1 Jeff Davis ‚Ä¶                     0               15 (((-104.9796 30.62936, -‚Ä¶\n 2 King County‚Ä¶                     0               15 (((-100.5187 33.83565, -‚Ä¶\n 3 Loving Coun‚Ä¶                     0               15 (((-103.9839 31.99411, -‚Ä¶\n 4 Kenedy Coun‚Ä¶                     0               15 (((-97.39839 26.86789, -‚Ä¶\n 5 McMullen Co‚Ä¶                     0               15 (((-98.80251 28.10305, -‚Ä¶\n 6 Edwards Cou‚Ä¶                     0               15 (((-100.7004 30.28828, -‚Ä¶\n 7 Terrell Cou‚Ä¶                     0               15 (((-102.5669 30.28327, -‚Ä¶\n 8 Donley Coun‚Ä¶                     0               15 (((-101.0908 34.74981, -‚Ä¶\n 9 Briscoe Cou‚Ä¶                     0               15 (((-101.4721 34.43408, -‚Ä¶\n10 Borden Coun‚Ä¶                     1                2 (((-101.6913 32.96184, -‚Ä¶"
  },
  {
    "objectID": "Assignment3.html#api-key",
    "href": "Assignment3.html#api-key",
    "title": "Assignment3",
    "section": "",
    "text": "census_api_key(\"326daf02214d3d3502dd97b6d473e77a82baf1c2\", install = FALSE)"
  },
  {
    "objectID": "Assignment3.html#explore-variables",
    "href": "Assignment3.html#explore-variables",
    "title": "Assignment3",
    "section": "",
    "text": "vars &lt;- load_variables(2023, \"acs5\", cache = TRUE)"
  },
  {
    "objectID": "Assignment3.html#view-a-few-example-codes",
    "href": "Assignment3.html#view-a-few-example-codes",
    "title": "Assignment3",
    "section": "",
    "text": "vars |&gt; dplyr::filter(grepl(\"^B19\", name)) |&gt; dplyr::slice_head(n = 10)\n\n# A tibble: 10 √ó 4\n   name        label                                concept            geography\n   &lt;chr&gt;       &lt;chr&gt;                                &lt;chr&gt;              &lt;chr&gt;    \n 1 B19001A_001 Estimate!!Total:                     Household Income ‚Ä¶ tract    \n 2 B19001A_002 Estimate!!Total:!!Less than $10,000  Household Income ‚Ä¶ tract    \n 3 B19001A_003 Estimate!!Total:!!$10,000 to $14,999 Household Income ‚Ä¶ tract    \n 4 B19001A_004 Estimate!!Total:!!$15,000 to $19,999 Household Income ‚Ä¶ tract    \n 5 B19001A_005 Estimate!!Total:!!$20,000 to $24,999 Household Income ‚Ä¶ tract    \n 6 B19001A_006 Estimate!!Total:!!$25,000 to $29,999 Household Income ‚Ä¶ tract    \n 7 B19001A_007 Estimate!!Total:!!$30,000 to $34,999 Household Income ‚Ä¶ tract    \n 8 B19001A_008 Estimate!!Total:!!$35,000 to $39,999 Household Income ‚Ä¶ tract    \n 9 B19001A_009 Estimate!!Total:!!$40,000 to $44,999 Household Income ‚Ä¶ tract    \n10 B19001A_010 Estimate!!Total:!!$45,000 to $49,999 Household Income ‚Ä¶ tract"
  },
  {
    "objectID": "Assignment3.html#parameters",
    "href": "Assignment3.html#parameters",
    "title": "Assignment3",
    "section": "",
    "text": "state_abbr &lt;- \"TX\"\ngeo_level  &lt;- \"county\"   # options: state, county, tract, block group\nmy_vars &lt;- c(home_value = \"B25077_001\", unemployment = \"B23025_005\")\nyear_acs   &lt;- 2023\nsurvey     &lt;- \"acs5\""
  },
  {
    "objectID": "Assignment3.html#download",
    "href": "Assignment3.html#download",
    "title": "Assignment3",
    "section": "",
    "text": "acs &lt;- get_acs(\n  geography = geo_level,\n  variables = my_vars,\n  state = state_abbr,\n  year = year_acs,\n  survey = survey,\n  geometry = TRUE\n)"
  },
  {
    "objectID": "Assignment3.html#wide-format-for-convenience",
    "href": "Assignment3.html#wide-format-for-convenience",
    "title": "Assignment3",
    "section": "",
    "text": "acs_wide &lt;- acs |&gt;\n  tidyr::pivot_wider(\n    id_cols = c(GEOID, NAME, geometry),\n    names_from = variable,\n    values_from = c(estimate, moe)\n  )"
  },
  {
    "objectID": "Assignment3.html#map",
    "href": "Assignment3.html#map",
    "title": "Assignment3",
    "section": "",
    "text": "ggplot(acs_wide) +\n  geom_sf(aes(fill = estimate_home_value), color = NA) +\n  scale_fill_viridis_c(name = \"Median Home Value\") +\n  labs(title = paste0(\"ACS \", year_acs, \" 5-year: Median Home Value ‚Äì \", state_abbr, \" (\", geo_level, \")\"),\n       caption = \"Source: U.S. Census Bureau via tidycensus\") +\n  theme_minimal() +\n  theme(text=element_text(family = \"Palatino\"),\n        plot.title = element_text(size = 14, face = \"bold\"),\n        plot.caption = element_text(size = 8), legend.position = \"inside\",\n        legend.position.inside = c(0.2, 0.15)) # move legend inside the chart"
  },
  {
    "objectID": "Assignment3.html#table-topbottom-by-poverty-count",
    "href": "Assignment3.html#table-topbottom-by-poverty-count",
    "title": "Assignment3",
    "section": "",
    "text": "top10 &lt;- acs_wide |&gt;\n  arrange(desc(estimate_unemployment)) |&gt;\n  select(NAME, estimate_unemployment, moe_unemployment) |&gt;\n  slice_head(n = 10)\n\nbottom10 &lt;- acs_wide |&gt;\n  arrange(estimate_unemployment) |&gt;\n  select(NAME, estimate_unemployment, moe_unemployment) |&gt;\n  slice_head(n = 10)\n\ntop10\n\nSimple feature collection with 10 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -106.6456 ymin: 26.03642 xmax: -94.9085 ymax: 33.43045\nGeodetic CRS:  NAD83\n# A tibble: 10 √ó 4\n   NAME         estimate_unemployment moe_unemployment                  geometry\n   &lt;chr&gt;                        &lt;dbl&gt;            &lt;dbl&gt;        &lt;MULTIPOLYGON [¬∞]&gt;\n 1 Harris Coun‚Ä¶                158268             4767 (((-94.97839 29.68365, -‚Ä¶\n 2 Dallas Coun‚Ä¶                 67955             2797 (((-97.03852 32.56, -97.‚Ä¶\n 3 Bexar Count‚Ä¶                 56081             2503 (((-98.80655 29.69071, -‚Ä¶\n 4 Tarrant Cou‚Ä¶                 53014             2568 (((-97.55053 32.56258, -‚Ä¶\n 5 Travis Coun‚Ä¶                 33873             2448 (((-98.15927 30.37665, -‚Ä¶\n 6 Hidalgo Cou‚Ä¶                 28197             1871 (((-98.58634 26.25824, -‚Ä¶\n 7 Collin Coun‚Ä¶                 25247             1528 (((-96.8441 32.98891, -9‚Ä¶\n 8 El Paso Cou‚Ä¶                 24609             1658 (((-106.6455 31.89867, -‚Ä¶\n 9 Fort Bend C‚Ä¶                 21887             1565 (((-96.08891 29.60166, -‚Ä¶\n10 Denton Coun‚Ä¶                 21327             1762 (((-97.39826 32.99996, -‚Ä¶\n\nbottom10\n\nSimple feature collection with 10 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -104.98 ymin: 26.598 xmax: -97.29015 ymax: 35.18326\nGeodetic CRS:  NAD83\n# A tibble: 10 √ó 4\n   NAME         estimate_unemployment moe_unemployment                  geometry\n   &lt;chr&gt;                        &lt;dbl&gt;            &lt;dbl&gt;        &lt;MULTIPOLYGON [¬∞]&gt;\n 1 Jeff Davis ‚Ä¶                     0               15 (((-104.9796 30.62936, -‚Ä¶\n 2 King County‚Ä¶                     0               15 (((-100.5187 33.83565, -‚Ä¶\n 3 Loving Coun‚Ä¶                     0               15 (((-103.9839 31.99411, -‚Ä¶\n 4 Kenedy Coun‚Ä¶                     0               15 (((-97.39839 26.86789, -‚Ä¶\n 5 McMullen Co‚Ä¶                     0               15 (((-98.80251 28.10305, -‚Ä¶\n 6 Edwards Cou‚Ä¶                     0               15 (((-100.7004 30.28828, -‚Ä¶\n 7 Terrell Cou‚Ä¶                     0               15 (((-102.5669 30.28327, -‚Ä¶\n 8 Donley Coun‚Ä¶                     0               15 (((-101.0908 34.74981, -‚Ä¶\n 9 Briscoe Cou‚Ä¶                     0               15 (((-101.4721 34.43408, -‚Ä¶\n10 Borden Coun‚Ä¶                     1                2 (((-101.6913 32.96184, -‚Ä¶"
  },
  {
    "objectID": "Assignment5.html",
    "href": "Assignment5.html",
    "title": "Assignment5",
    "section": "",
    "text": "library(purrr)\nlibrary(jsonlite)\nlibrary(data.table)\nlibrary(readr)\nlibrary(stringr)"
  },
  {
    "objectID": "Assignment5.html#read-the-csv-from-govinfo.",
    "href": "Assignment5.html#read-the-csv-from-govinfo.",
    "title": "Assignment5",
    "section": "1) Read the CSV from govinfo.",
    "text": "1) Read the CSV from govinfo.\n\ngovfiles &lt;- read.csv(\"govinfo_foreign_affairs_118.csv\")\n\npdf_govfiles_url &lt;- govfiles$pdfLink[1:10]\npdf_govfiles_id  &lt;- govfiles$packageId[1:10]\n\nsave_dir &lt;- \"C:/Users/Luna/OneDrive/Documents/rluna\"\ndir.create(save_dir, showWarnings = FALSE)\n\nlibrary(purrr)\n\ndownload_govfiles_pdf &lt;- function(url, id) {\n  tryCatch({\n    destfile &lt;- paste0(save_dir, \"/\", id, \".pdf\")\n    \n    download.file(url, destfile, mode = \"wb\")\n    \n    Sys.sleep(runif(1, 1, 3)) \n    \n    return(paste(\"Successfully downloaded:\", id))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", id))\n  })\n}\n\nresults &lt;- purrr::map_chr(\n  1:10,\n  ~ download_govfiles_pdf(pdf_govfiles_url[.x], pdf_govfiles_id[.x])\n)\n\nresults"
  },
  {
    "objectID": "Assignment5.html#reporting",
    "href": "Assignment5.html#reporting",
    "title": "Assignment5",
    "section": "2) Reporting",
    "text": "2) Reporting\n\nHow usable is the scraped data?\n\nThe data I scraped from GovInfo is usable. The CSV has a lot of helpful information like the package IDs and PDF links, but some columns aren‚Äôt very clean.. It works fine for downloading files, but it would need extra cleaning before doing any real analysis of the text.\n\nHow to improve?\n\nThe process could be improved by cleaning the data earlier. Adding more checks during the download (like confirming each file saved correctly) would also make it smoother. Overall, it works, but a little more automation would make the whole thing easier."
  },
  {
    "objectID": "FinalProject.html",
    "href": "FinalProject.html",
    "title": "Spatial Data Science Final Project",
    "section": "",
    "text": "üìÑ Download Paper"
  },
  {
    "objectID": "FinalProject.html#environmental-risk-index-for-commercial-properties-in-richardson-tx",
    "href": "FinalProject.html#environmental-risk-index-for-commercial-properties-in-richardson-tx",
    "title": "Spatial Data Science Final Project",
    "section": "",
    "text": "üìÑ Download Paper"
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Resume",
    "section": "",
    "text": "Raquel Luna Resume\nüìÑ Download Resume"
  },
  {
    "objectID": "sub1.html",
    "href": "sub1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Objective: Identify data or model problems using visualization.\n\n\n\n\n\n\n\n\n\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n\n\n\n\n\n\n\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n\n\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n\n\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602"
  },
  {
    "objectID": "sub1.html#data-visualization",
    "href": "sub1.html#data-visualization",
    "title": "Assignment 1",
    "section": "",
    "text": "Objective: Identify data or model problems using visualization.\n\n\n\n\n\n\n\n\n\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n\n\n\n\n\n\n\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n\n\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n\n\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602"
  },
  {
    "objectID": "sub1.html#google-generative-art.-cite-some-examples.",
    "href": "sub1.html#google-generative-art.-cite-some-examples.",
    "title": "Assignment 1",
    "section": "2. Google ‚ÄúGenerative Art‚Äù. Cite some examples.",
    "text": "2. Google ‚ÄúGenerative Art‚Äù. Cite some examples.\n\n\n\nChandra Michelle, 2019. Endless Forms Most Beautiful - Generative Spirograph Prints. https://www.michellechandra.com/generative-design-2/generative-art-spirograph-prints/\n\n\n\n\n\nManolo Gamboa Naon, https://aiartists.org/generative-art-design"
  },
  {
    "objectID": "sub1.html#fall",
    "href": "sub1.html#fall",
    "title": "Assignment 1",
    "section": "3. Fall",
    "text": "3. Fall\n\nObjective: Create Graphics with R"
  },
  {
    "objectID": "sub1.html#chart-critique",
    "href": "sub1.html#chart-critique",
    "title": "Assignment 1",
    "section": "4. Chart Critique",
    "text": "4. Chart Critique\n\n\n\nChart\n\n\nThe chart titled ‚ÄúWhat English do you Speak‚Äù presents the user‚Äôs linguistic accent profile, with ‚ÄúAmerican‚Äù being the predominant result at 85%, followed by accents like Canadian (78%), Australian (61%), British (55%), New Zealand (54%), and South African (45%). The horizontal bars effectively display the relative strength of each accent, making it easy to visually compare the different results. The description for the American accent adds a humorous and engaging touch, referencing regional dialects like ‚ÄúNo‚Äôth Ca‚Äôlina‚Äù and ‚ÄúNew Yawk,‚Äù which adds a lighthearted tone to the quiz‚Äôs outcome and makes it more relatable to an American audience.However, the chart lacks detail regarding the methodology behind these results. It doesn‚Äôt explain how the percentages were calculated, leaving the reader without an understanding of what criteria were used to determine the accent breakdown. Additionally, there seems to be a strong bias towards American English, both in the dominant percentage and the tone of the accompanying text. The focus on American regional dialects, while entertaining, might make the quiz less relevant to users from non-American English-speaking backgrounds. Moreover, the detailed description is only provided for the American accent, while other accents, such as Canadian or Australian, are not given any context or explanation. Offering descriptions for the other accent results and more information about how the results were derived would provide a more comprehensive and balanced experience for users."
  },
  {
    "objectID": "sub3.html",
    "href": "sub3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "Call:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165"
  },
  {
    "objectID": "sub3.html#rerun-anscombe01.r",
    "href": "sub3.html#rerun-anscombe01.r",
    "title": "Assignment 3",
    "section": "",
    "text": "Call:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165"
  },
  {
    "objectID": "sub3.html#comparing-regression-models",
    "href": "sub3.html#comparing-regression-models",
    "title": "Assignment 3",
    "section": "Comparing Regression Models",
    "text": "Comparing Regression Models\nAnscombe‚Äôs Quartet consists of four datasets that have nearly identical summary statistics‚Äîmean, variance, correlation, and regression line slope‚Äîyet exhibit very different distributions and visual patterns. By fitting a linear regression model to each dataset (lm(y ~ x)), we can examine how these similarities hold across the datasets, despite their differing behaviors when plotted.\nKey Observations from Regression Summaries: Similar regression coefficients: Each dataset shows very similar regression coefficients for the intercept and slope, confirming that the regression lines are statistically similar. For instance, the slope of the regression lines across all four datasets hovers around the same value (close to 0.5), and the intercepts remain near 3. Identical R-squared values: The R-squared values, which represent how well the model fits the data, are nearly identical across the four datasets. This means that, in terms of variance explained by the linear model, the datasets appear similar. Despite these statistical similarities, the underlying data in each dataset is quite different, which becomes apparent only through plotting. ‚Äî\n\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165"
  },
  {
    "objectID": "sub3.html#finetuning-the-charts",
    "href": "sub3.html#finetuning-the-charts",
    "title": "Assignment 3",
    "section": "Finetuning the charts",
    "text": "Finetuning the charts"
  },
  {
    "objectID": "sub3.html#i-decided-to-explore-with-some-different-pch-font-and-colors-for-the-charts.-i-tried-not-using-default-colors-and-use-my-own-plotting-characters-for-experimintation-until-i-can-find-my-own-style.",
    "href": "sub3.html#i-decided-to-explore-with-some-different-pch-font-and-colors-for-the-charts.-i-tried-not-using-default-colors-and-use-my-own-plotting-characters-for-experimintation-until-i-can-find-my-own-style.",
    "title": "Assignment 3",
    "section": "I decided to explore with some different PCH, font and colors for the charts. I tried not using default colors and use my own plotting characters for experimintation, until I can find my own style.",
    "text": "I decided to explore with some different PCH, font and colors for the charts. I tried not using default colors and use my own plotting characters for experimintation, until I can find my own style."
  },
  {
    "objectID": "sub5.html#barchart-horizontal",
    "href": "sub5.html#barchart-horizontal",
    "title": "Assignment 5",
    "section": "Barchart (Horizontal)",
    "text": "Barchart (Horizontal)\n\n\n                        City Fitness_Fee\n1 Houston..TX..United.States       10.14\n2  Dallas..TX..United.States        2.99\n3  Austin..TX..United.States       14.60"
  },
  {
    "objectID": "sub5.html#pie-chart",
    "href": "sub5.html#pie-chart",
    "title": "Assignment 5",
    "section": "Pie Chart",
    "text": "Pie Chart\n\n\n                        City Fitness_Fee\n1 Houston..TX..United.States       10.14\n2  Dallas..TX..United.States        2.99\n3  Austin..TX..United.States       14.60"
  },
  {
    "objectID": "sub5.html#boxplot",
    "href": "sub5.html#boxplot",
    "title": "Assignment 5",
    "section": "Boxplot",
    "text": "Boxplot\n\n\n                        City Fitness_Fee\n1 Houston..TX..United.States       10.14\n2  Dallas..TX..United.States        2.99\n3  Austin..TX..United.States       14.60"
  },
  {
    "objectID": "sub5.html#scatterplot",
    "href": "sub5.html#scatterplot",
    "title": "Assignment 5",
    "section": "Scatterplot",
    "text": "Scatterplot\n\n\n                        City Fitness_Fee\n1 Houston..TX..United.States       10.14\n2  Dallas..TX..United.States        2.99\n3  Austin..TX..United.States       14.60"
  }
]